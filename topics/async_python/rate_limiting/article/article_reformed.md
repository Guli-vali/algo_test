# Асинхронное лимитирование запросов: путь от наивного async к production-ready решению

Когда я впервые столкнулся с задачей массовой выгрузки данных из внешнего API, мне казалось, что **async решит всё**.
`await`, `gather`, семафоры — инструменты знакомые, документация прочитана.

На практике же я собрал **впечатляющую коллекцию граблей**:
от синхронного async-кода до DDoS самому себе.

В этой статье — не «правильный способ», а **путь**, который я прошёл, и архитектура, к которой пришёл как к **стабильной золотой середине для своего кейса**.

Это **не серебряная пуля**, но это решение, которое спокойно живёт в продакшене.

---

## Грабли №1: async, который работает синхронно

Первый вариант выглядел примерно так:

```python
async def fetch_all_pages():
    page = 1
    while True:
        response = await fetch(f"/resource?page={page}")
        process(response)
        page += 1
```

Формально:

* `async`
* `await`
* никаких `sleep`

Фактически:

* **один запрос за раз**
* каждый следующий ждёт предыдущий

> **Async-код не становится параллельным автоматически.
> Если каждый запрос ждёт завершения предыдущего —
> это синхронная программа в async-синтаксисе.**

Для небольших объёмов это незаметно.
Для миллионов записей — катастрофа по времени.

---

## Грабли №2: «давайте просто gather всё»

Следующий логичный шаг — `asyncio.gather`:

```python
tasks = [fetch(page) for page in range(1, 1000)]
results = await asyncio.gather(*tasks)
```

На бумаге — идеально:

* максимальный параллелизм
* минимальное время

На практике:

* burst из сотен запросов
* отсутствие rate limit
* рост памяти
* 429 и нестабильность

> **`asyncio.gather` без ограничений —
> это не параллелизм, а хаос.**

Иногда такой код «работает».
Но это именно **удача**, а не архитектура.

---

## Грабли №3: semaphore как rate limit

Дальше появляется семафор:

```python
semaphore = Semaphore(10)
```

Интуитивное ожидание:

> «Ну значит 10 запросов в секунду»

Реальность:

* семафор ограничивает **одновременность**
* скорость зависит от **latency API**
* поведение непредсказуемо

При медленном API — всё «почти нормально».
При быстром — мгновенный бан.

> **Semaphore регулирует ширину трубы.
> Rate limiter регулирует поток воды.**

Семафор нужен.
Но **не для контроля скорости**.

---

## Первый адекватный шаг: worker pool

Момент, когда код перестаёт быть хаотичным — это переход к **ограниченному пулу воркеров**:

* нет burst-нагрузки
* контролируется параллелизм
* можно стримить результаты
* система начинает «дышать»

Это первый вариант, который **не стыдно показать**.

> **Worker pool даёт контроль над параллелизмом,
> но не контролирует скорость.**

И здесь становится очевидно:
**параллелизм и rate limit — разные оси**.

---

## Кульминация: rate limiter + worker pool

Финальная архитектура для моего кейса:

* **Rate limiter** — контролирует, как часто стартуют запросы
* **Worker pool / semaphore** — контролирует количество запросов в полёте
* Очереди — дают backpressure
* Результаты — стримятся, а не копятся в памяти

Это:

* стабильно
* предсказуемо
* масштабируемо
* не зависит от случайной скорости API

> **Общее время выгрузки определяется количеством запросов,
> а не временем ответа — если архитектура правильная.**

---

## Почему это не серебряная пуля

Важно честно сказать:

* для ultra-low latency API подход может быть избыточен
* для простых задач хватит последовательной загрузки
* разные API считают лимиты по-разному
* иногда нужен adaptive rate limit

Это **не универсальный рецепт**, а **золотая середина под конкретные ограничения**:

* миллионы записей
* жёсткий rate limit
* нестабильная latency
* продакшен-нагрузка

---

## Что дальше

В следующей статье я планирую сделать **обзор всех стратегий асинхронного лимитирования**:

* sequential async
* burst gather
* semaphore-only
* bounded gather
* worker pool
* token bucket
* leaky bucket
* sliding window
* adaptive rate limit

С реальными кейсами, плюсами и минусами —
без «единственно правильного решения».
